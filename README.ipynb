{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Object Detection with Yolo Algorithm\n",
    "You Only Look Once: Unified, Real-Time Object Detection\n",
    "\n",
    "YOLO papers: Redmon et al., 2016 (https://arxiv.org/abs/1506.02640) and Redmon and Farhadi, 2016 (https://arxiv.org/abs/1612.08242).\n",
    "\n",
    "YOLO(You only look once) is a popolar algorithm, as it achieves high accuracy and also run in real-time. It require only one forwared propogation pass through the network to make predictions. It outputs recongnized object with Bounding Boxes and percentage accuracy in real-time. It can also determine any number of Objects/Classes at the same time.\n",
    "\n",
    "Downside of Yolo: Resource Expensive(Recommended to train only on GPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Input**: \n",
    "- 1) Set of Images for Training,a Single Image may have multiple objects to detect(.png)\n",
    "- 2) Set of Annotation defined with Bounding Boxes(.xml)\n",
    "\n",
    "**Output**\n",
    "List of Bounding Boxes along with recongnized Object. Each bounding box is represented by: 6 numbers $(p_c, x_min, y_min, x_max,y_max, c)$\n",
    "- $p_c$ : Confidence over the Object\n",
    "- $xmin$ : x-min point of Bounding Box.\n",
    "- $ymin$ : y-min point of Bounding Box.\n",
    "- $xmax$ : x-max point of Bounding Box.\n",
    "- $ymax$ : y-max point of Bounding Box.\n",
    "- c      : Class of Detected object(A vector of number of Classes).\n",
    "\n",
    "OR y can be defined as:\n",
    "\n",
    "- $p_c$ : Confidence over the Object\n",
    "- $b_x$ : x point, center of Bounding Box.\n",
    "- $b_y$ : y point, center of Bounding Box.\n",
    "- $b_h$ : Height of Bounding Box.\n",
    "- $b_w$ : Width  Bounding Box.\n",
    "- c      : Class of Detected object(A vector of number of Classes).\n",
    "\n",
    "<img src=\"notebook_images/box_label.png\" style=\"width:500px;height:250;\">\n",
    "<caption><center> <u> **Figure 1** </u>: **Definition of a box**<br> </center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It apply a single neural network to the full image. This network divides the image into regions and predicts bounding boxes and probabilities for each region. These bounding boxes are weighted by the predicted probabilities. It consist of Convolution Layers and Pooling Layer.\n",
    "<img src=\"notebook_images/anchor_map.png\" style=\"width:200px;height:200;\">\n",
    "<caption><center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-max Suppression Algo\n",
    "- Problem: The output produces by Convolution layers results in multiple boxes over a Single object.\n",
    "- Each output box has a predicted class and probabily value.\n",
    "- It Discard all boxes with probility less than p_c(Ex < 0.6)\n",
    "- It Discard According to its IOU value.\n",
    "<u><img src=\"notebook_images/iou.png\" style=\"width:500px;height:400;\">\n",
    "<caption><center> <u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Layer:\n",
    "Convolutional layers apply a convolution operation to the input, passing the result to the next layer. The convolution emulates the response of an individual neuron to visual stimuli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Parameter Sharing:** A single feature detector( ie. filter) is used by Complete Image and each RGB channel, by multiplying the filters values with Image.\n",
    "\n",
    "**Sparsity of Connections:** Each output layer depends only on small number of inputs. As first element of output (ie. output[0][0]) layer depends only on few elememts of input.\n",
    "<img src=\"notebook_images/filter1.png\" style=\"width:800px;height:800;\">\n",
    "<caption><center>\n",
    "\n",
    "The number Weights in a NN layer that will learn, is defined by number of filter in previous layer and does not depends on size of input Image.\n",
    "Example : if a layer has 7 filters, then the Input of next NN layer will have dimension of (height, Width, 7 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer (Max Pooling)\n",
    "Max pooling uses the maximum value from each of a cluster of neurons at the prior layer. \n",
    "Hyperparameters: f,s\n",
    "- f: filter size\n",
    "- s: Stride\n",
    "\n",
    "It does not have any Parameter to learn as in Convolution layer.\n",
    "It is computed for each channel.\n",
    "<img src=\"notebook_images/pool.png\" style=\"width:800px;height:800;\">\n",
    "<caption><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparmeters\n",
    "\n",
    "**Padding:** It increases the size of Image by adding some pixel around the border of image. It is required, as while filtering the image it may loose the information around the borders.\n",
    "\n",
    "**Filter:** Each filter detects feauture in a Image. (such as Vertical line/ Lines at different Angles) in terms of pixels in a Image.\n",
    "Example for Vertical Edge Detection:\n",
    "<img src=\"notebook_images/vertical.png\" style=\"width:800px;height:800;\">\n",
    "<caption><center>\n",
    "\n",
    "**Size:** Batch Size for Training\n",
    "**Strides:** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Preparing Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download images for the DataSet and save images in \"/Data/Images\"\n",
    "- To create Annotations file Use Application to Draw Bounding Box, over the image to selecting over the object and save according its class.(\"/Data/Annotations\"\n",
    "App : https://github.com/tzutalin/labelImg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Example for Input Array: (m , Height, Weight, channels = 3)\n",
    "- m : Number of Training Example/Images\n",
    "- Height: Height of each image\n",
    "- Width:  Width of each image\n",
    "- channels: Each image has RGB layer vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring the model\n",
    "\n",
    "It require pre-trained weight to initialize model training, which can be donloaded from \"https://pjreddie.com/media/files/tiny-yolo-voc.weights\"\n",
    "\n",
    "and Save the .weights file under in the **\"/bin\"** directory.\n",
    "\n",
    "Under the directory **\"/cfg/\"** subdirectory.\n",
    "- /yolo.cfg  :is based on the extraction network. It process images at 45 fps.\n",
    "- /yolo-small.cfg/ :has smaller fully connected layers and It uses very less memory. It process images at 50fps.\n",
    "- /yolo-tiny.cfg :is much smaller and process images at 155 fps.\n",
    "\n",
    "For training and defining model, it require only single .cfg file, which defines the number and Sequence of Layer with their Hyperparameters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a Complete model\n",
    "<img src=\"notebook_images/modelexample.png\" style=\"width:600px;height:400;\">\n",
    "<caption><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory field to change \n",
    "#### In **cfg/tiny-yolo-volc-1c.cfg**\n",
    "\n",
    "1) Change classes in the [region] layer (the last layer) to the number of classes, you want to train. (To find only one object, like just car set classes=1).\n",
    "\n",
    "[region]\n",
    "classes=1\n",
    "\n",
    "2) The number of filers in the [colvolution] layer (the second last layer) to num * (classes + 5) = 5*(1+5) => 30\n",
    "\n",
    "[convolutional]\n",
    "filters=30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In **labels.txt**\n",
    "\n",
    "Define the number of classes/Objects with their Actual names.\n",
    "- car\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command to train the model\n",
    "\n",
    "\n",
    "> python flow --model cfg/tiny-yolo-voc-1c.cfg  --load bin/tiny-yolo-voc.weights  --train  --annotation data/annotations --dataset data/images  --gpu 0.9  --epoch 300\n",
    "\n",
    "> python flow  --model [.cfg directory]  --load [.weights]  --train  --annotations [.xml]  --dataset [.png]  --gpu [0.1 to 1.0]  --epoch [training iterations]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- First try without using **--gpu** flag, as tensorflow gpu version might not be properly installed on your system.\n",
    "- This will start the model training and while training after particular iteration, it will save a **\".weights\"** files in **/bin** directory. For predict method use the last generated file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume training with last Checkpoint (Pre-trained model)\n",
    "flow --train --model cfg/yolo-new.cfg --load -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with Camera or Video file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For CPU\n",
    "> python flow --model cfg/tiny-yolo-voc-1c.cfg --load bin/yolo-new.weights --demo videofile.avi\n",
    "\n",
    "> python flow --model cfg/tiny-yolo-voc-1c.cfg --load 300 --demo videofile.avi\n",
    "\n",
    "- --load flag iteration last ckeckpoint save in /ckpt/\n",
    "\n",
    "### For GPU\n",
    "> python flow --model cfg/tiny-yolo-voc-1c.cfg --load bin/yolo-new.weights --demo videofile.avi --gpu 0.8\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To use webcam for live predicted bounding box, **replace(\"videofile.avi\",\"camera\")** \n",
    "- To save as video with predicted bounding box, add **--saveVideo** option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For using model in python script"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "from darkflow.net.build import TFNet\n",
    "import cv2\n",
    "options = {\"model\": \"cfg/yolo.cfg\", \"load\": \"bin/yolo.weights\", \"threshold\": 0.1}\n",
    "tfnet = TFNet(options)\n",
    "\n",
    "imgcv = cv2.imread(\"./sample_img/sample_dog.jpg\")\n",
    "result = tfnet.return_predict(imgcv)\n",
    "print(result)\n",
    "\n",
    "#This will return a json representing each detected object probability with bounding box dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
